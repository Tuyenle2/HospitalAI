{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3da36217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.60489\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.61412\tvalidation_1-error:0.30818\n",
      "[1]\tvalidation_0-logloss:0.60146\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.61122\tvalidation_1-error:0.30818\n",
      "[2]\tvalidation_0-logloss:0.59826\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.60769\tvalidation_1-error:0.30818\n",
      "[3]\tvalidation_0-logloss:0.59559\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.60504\tvalidation_1-error:0.30818\n",
      "[4]\tvalidation_0-logloss:0.59231\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.60356\tvalidation_1-error:0.30818\n",
      "[5]\tvalidation_0-logloss:0.58957\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.60173\tvalidation_1-error:0.30818\n",
      "[6]\tvalidation_0-logloss:0.58769\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.60132\tvalidation_1-error:0.30818\n",
      "[7]\tvalidation_0-logloss:0.58495\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.59943\tvalidation_1-error:0.30818\n",
      "[8]\tvalidation_0-logloss:0.58281\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.59715\tvalidation_1-error:0.30818\n",
      "[9]\tvalidation_0-logloss:0.58037\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.59535\tvalidation_1-error:0.30818\n",
      "[10]\tvalidation_0-logloss:0.57800\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.59312\tvalidation_1-error:0.30818\n",
      "[11]\tvalidation_0-logloss:0.57547\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.59032\tvalidation_1-error:0.30818\n",
      "[12]\tvalidation_0-logloss:0.57348\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58931\tvalidation_1-error:0.30818\n",
      "[13]\tvalidation_0-logloss:0.57070\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58810\tvalidation_1-error:0.30818\n",
      "[14]\tvalidation_0-logloss:0.56890\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58689\tvalidation_1-error:0.30818\n",
      "[15]\tvalidation_0-logloss:0.56700\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58578\tvalidation_1-error:0.30818\n",
      "[16]\tvalidation_0-logloss:0.56511\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58573\tvalidation_1-error:0.30818\n",
      "[17]\tvalidation_0-logloss:0.56381\tvalidation_0-error:0.29717\tvalidation_1-logloss:0.58596\tvalidation_1-error:0.30818\n",
      "[18]\tvalidation_0-logloss:0.56223\tvalidation_0-error:0.29403\tvalidation_1-logloss:0.58525\tvalidation_1-error:0.30818\n",
      "[19]\tvalidation_0-logloss:0.56042\tvalidation_0-error:0.29403\tvalidation_1-logloss:0.58370\tvalidation_1-error:0.30818\n",
      "[20]\tvalidation_0-logloss:0.55909\tvalidation_0-error:0.29403\tvalidation_1-logloss:0.58302\tvalidation_1-error:0.30189\n",
      "[21]\tvalidation_0-logloss:0.55802\tvalidation_0-error:0.29245\tvalidation_1-logloss:0.58195\tvalidation_1-error:0.30189\n",
      "[22]\tvalidation_0-logloss:0.55616\tvalidation_0-error:0.28616\tvalidation_1-logloss:0.58047\tvalidation_1-error:0.30189\n",
      "[23]\tvalidation_0-logloss:0.55470\tvalidation_0-error:0.28302\tvalidation_1-logloss:0.57955\tvalidation_1-error:0.30189\n",
      "[24]\tvalidation_0-logloss:0.55351\tvalidation_0-error:0.27830\tvalidation_1-logloss:0.57852\tvalidation_1-error:0.30189\n",
      "[25]\tvalidation_0-logloss:0.55222\tvalidation_0-error:0.28145\tvalidation_1-logloss:0.57745\tvalidation_1-error:0.30189\n",
      "[26]\tvalidation_0-logloss:0.55119\tvalidation_0-error:0.28145\tvalidation_1-logloss:0.57747\tvalidation_1-error:0.30189\n",
      "[27]\tvalidation_0-logloss:0.55014\tvalidation_0-error:0.28145\tvalidation_1-logloss:0.57642\tvalidation_1-error:0.30189\n",
      "[28]\tvalidation_0-logloss:0.54889\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57638\tvalidation_1-error:0.30189\n",
      "[29]\tvalidation_0-logloss:0.54770\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57590\tvalidation_1-error:0.30189\n",
      "[30]\tvalidation_0-logloss:0.54631\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57477\tvalidation_1-error:0.30189\n",
      "[31]\tvalidation_0-logloss:0.54485\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57377\tvalidation_1-error:0.30189\n",
      "[32]\tvalidation_0-logloss:0.54393\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57280\tvalidation_1-error:0.30189\n",
      "[33]\tvalidation_0-logloss:0.54302\tvalidation_0-error:0.27987\tvalidation_1-logloss:0.57252\tvalidation_1-error:0.30189\n",
      "[34]\tvalidation_0-logloss:0.54195\tvalidation_0-error:0.27830\tvalidation_1-logloss:0.57230\tvalidation_1-error:0.30189\n",
      "[35]\tvalidation_0-logloss:0.54099\tvalidation_0-error:0.27673\tvalidation_1-logloss:0.57280\tvalidation_1-error:0.30189\n",
      "[36]\tvalidation_0-logloss:0.54027\tvalidation_0-error:0.27516\tvalidation_1-logloss:0.57341\tvalidation_1-error:0.29560\n",
      "[37]\tvalidation_0-logloss:0.53925\tvalidation_0-error:0.27516\tvalidation_1-logloss:0.57347\tvalidation_1-error:0.29560\n",
      "[38]\tvalidation_0-logloss:0.53845\tvalidation_0-error:0.27516\tvalidation_1-logloss:0.57373\tvalidation_1-error:0.29560\n",
      "[39]\tvalidation_0-logloss:0.53767\tvalidation_0-error:0.27358\tvalidation_1-logloss:0.57286\tvalidation_1-error:0.29560\n",
      "Độ chính xác: 0.7044\n",
      "Log Loss: 0.5729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XUAN TUYEN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [20:00:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('Benh_nhom1.csv')\n",
    "\n",
    "# Calculate age\n",
    "current_year = 2025\n",
    "df['Tuoi'] = current_year - df['Nam_Sinh']\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'Tuoi', 'So_Lan_Mang_Thai', 'So_Lan_Sinh_Con',\n",
    "    'Tien_Su_Tranh_Thai', 'Tien_Su_Gia_Dinh', 'Tien_Su_MangThai',\n",
    "    'Nghe_Nghiep', 'Trinh_Do_Hoc_Van', 'Tinh_Trang_hon_nhan'\n",
    "]\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['Nghe_Nghiep', 'Trinh_Do_Hoc_Van', 'Tinh_Trang_hon_nhan', 'Tien_Su_Tranh_Thai']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = ['Tuoi', 'So_Lan_Mang_Thai', 'So_Lan_Sinh_Con']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[selected_columns]\n",
    "y = df['CoBenh'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model \n",
    "model = XGBClassifier(\n",
    "    n_estimators=40,\n",
    "    max_depth=3,  \n",
    "    learning_rate=0.05,  \n",
    "    reg_lambda=1.0,  \n",
    "    reg_alpha=0.5,  \n",
    "    subsample=0.8,  \n",
    "    colsample_bytree=0.8,  \n",
    "    random_state=42,\n",
    "    eval_metric=[\"logloss\", \"error\"]\n",
    ")\n",
    "\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "model.fit(X_train, y_train, eval_set=eval_set)\n",
    "# Extract evaluation results\n",
    "results = model.evals_result()\n",
    "epochs = range(len(results['validation_0']['logloss']))\n",
    "train_loss = results['validation_0']['logloss']\n",
    "val_loss = results['validation_1']['logloss']\n",
    "train_error = results['validation_0']['error']\n",
    "val_error = results['validation_1']['error']\n",
    "train_accuracy = [1 - x for x in train_error]\n",
    "val_accuracy = [1 - x for x in val_error]\n",
    "\n",
    "# Calculate final metrics on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "final_accuracy = accuracy_score(y_val, y_pred)\n",
    "final_log_loss = log_loss(y_val, model.predict_proba(X_val))\n",
    "\n",
    "print(f\"Độ chính xác: {final_accuracy:.4f}\")\n",
    "print(f\"Log Loss: {final_log_loss:.4f}\")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Loss trong quá trình huấn luyện và validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Độ chính xác trong quá trình huấn luyện và validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.close()\n",
    "\n",
    "# Save model using XGBoost's save_model\n",
    "model.save_model('my_trained_model.model')\n",
    "\n",
    "# Save preprocessors\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
